{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /venv/main/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /venv/main/lib/python3.10/site-packages (from rouge-score) (2.2.2)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from rouge-score) (2.1.2)\n",
      "Requirement already satisfied: nltk in /venv/main/lib/python3.10/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /venv/main/lib/python3.10/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /venv/main/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /venv/main/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /venv/main/lib/python3.10/site-packages (from nltk->rouge-score) (2024.11.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /venv/main/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /venv/main/lib/python3.10/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /venv/main/lib/python3.10/site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/main/lib/python3.10/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: transformers in /venv/main/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /venv/main/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, get_linear_schedule_with_warmup\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_len   = 512      \n",
    "max_tgt_len   = 128      \n",
    "batch_size    = 8\n",
    "epochs        = 25\n",
    "lr            = 0.0001\n",
    "weight_decay  = 0.01\n",
    "grad_clip     = 1.0\n",
    "beam_size     = 4\n",
    "warmup_ratio  = 0.1\n",
    "checkpoint_dir = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    src = tokenizer(batch['text'], max_length=max_src_len, truncation=True, padding='max_length')\n",
    "    tgt = tokenizer(batch['target'], max_length=max_tgt_len, truncation=True, padding='max_length')\n",
    "    \n",
    "    return {'input_ids': src.input_ids, 'attention_mask': src.attention_mask, 'labels': tgt.input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(dataset_path):\n",
    "    splits = load_dataset(dataset_path, split={\"train\":\"train\",\"validation\":\"validation\"})\n",
    "    tokenized = splits.map(tokenize, batched=True)\n",
    "    tokenized.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "    train_loader = DataLoader(tokenized['train'], batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True,persistent_workers=True)\n",
    "    valid_loader = DataLoader(tokenized['validation'], batch_size=batch_size, num_workers=5, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = get_loaders(\"nlplabtdtu/xlsum_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 19 20:28:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:81:00.0 Off |                  Off |\n",
      "|  0%   41C    P8             10W /  450W |       2MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.pos = nn.Parameter(torch.zeros(max_len, d_model))\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pos[:seq_len].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSummarizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, enc_layers=3, dec_layers=3, dim_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed       = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc     = TrainablePositionalEncoding(d_model)\n",
    "        self.pos_dec     = TrainablePositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, enc_layers, dec_layers, dim_ff, dropout, activation='gelu', batch_first=True)\n",
    "        self.out_proj    = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_attention_mask=None, tgt_attention_mask=None):\n",
    "\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        src_kpm = (src == pad_id)\n",
    "        tgt_kpm = (tgt == pad_id)\n",
    "        \n",
    "        src_emb = self.pos_enc(self.embed(src) * math.sqrt(self.embed.embedding_dim))\n",
    "        tgt_emb = self.pos_dec(self.embed(tgt) * math.sqrt(self.embed.embedding_dim))\n",
    "        \n",
    "        size = tgt.size(1)\n",
    "        tgt_mask = torch.triu(torch.full((size, size), True,dtype=torch.bool), 1).to(device)\n",
    "        \n",
    "        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_kpm,\n",
    "                               tgt_key_padding_mask=tgt_kpm,\n",
    "                               memory_key_padding_mask=src_kpm\n",
    "                              )\n",
    "        \n",
    "        return self.out_proj(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSummarizer(\n",
       "  (embed): Embedding(50265, 512)\n",
       "  (pos_enc): TrainablePositionalEncoding()\n",
       "  (pos_dec): TrainablePositionalEncoding()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out_proj): Linear(in_features=512, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_loader) * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "    \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, scheduler, device, vocab_size, pad_token_id, grad_clip=1.0):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        \n",
    "        src = batch['input_ids'].to(device, non_blocking=True)\n",
    "        src_attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        tgt = batch['labels'].to(device, non_blocking=True)\n",
    "        \n",
    "        tgt_inp, tgt_lbl = tgt[:, :-1], tgt[:, 1:]\n",
    "        \n",
    "        tgt_attention_mask = (tgt_inp != pad_token_id).to(device, non_blocking=True)\n",
    "        \n",
    "        loss_mask = (tgt_lbl != pad_token_id).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(\n",
    "            src=src,\n",
    "            tgt=tgt_inp,\n",
    "            src_attention_mask=src_attention_mask,\n",
    "            tgt_attention_mask=tgt_attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = criterion(logits.reshape(-1, vocab_size), tgt_lbl.reshape(-1))\n",
    "        \n",
    "        masked_loss = (loss * loss_mask.reshape(-1)).sum() / max(loss_mask.sum(), 1)\n",
    "        \n",
    "        masked_loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        current_loss = masked_loss.item()\n",
    "        total_loss += current_loss\n",
    "        progress_bar.set_postfix({\"loss\": f\"{current_loss:.4f}\"})\n",
    "        \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, device, vocab_size, pad_token_id, tokenizer):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in progress_bar:\n",
    "            src = batch['input_ids'].to(device, non_blocking=True)\n",
    "            src_attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            tgt = batch['labels'].to(device, non_blocking=True)\n",
    "            tgt_inp, tgt_lbl = tgt[:, :-1], tgt[:, 1:]\n",
    "            \n",
    "            tgt_attention_mask = (tgt_inp != pad_token_id).to(device, non_blocking=True)\n",
    "            \n",
    "            loss_mask = (tgt_lbl != pad_token_id).float()\n",
    "            \n",
    "            logits = model(\n",
    "                src=src,\n",
    "                tgt=tgt_inp,\n",
    "                src_attention_mask=src_attention_mask,\n",
    "                tgt_attention_mask=tgt_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(logits.reshape(-1, vocab_size), tgt_lbl.reshape(-1))\n",
    "            masked_loss = (loss * loss_mask.reshape(-1)).sum() / max(loss_mask.sum(), 1)\n",
    "            total_loss += masked_loss.item()\n",
    "            \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_loader, val_loader, vocab_size, tokenizer, optimizer, criterion, scheduler, pad_token_id, grad_clip):\n",
    "    \n",
    "    model = model.to(device, non_blocking=True)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            vocab_size=vocab_size,\n",
    "            pad_token_id=pad_token_id,\n",
    "            grad_clip=grad_clip\n",
    "        )\n",
    "        \n",
    "        val_loss = evaluate(\n",
    "            model=model,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            vocab_size=vocab_size,\n",
    "            pad_token_id=pad_token_id,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss\n",
    "                    }, f\"{checkpoint_dir}/transformer_epoch_{epoch+1}.pt\")\n",
    "        print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src_ids, src_mask=None, max_len=100, beam_size=5, \n",
    "                       device='cpu', tokenizer=None, length_penalty=1.0, \n",
    "                       early_stopping=True):\n",
    "    \n",
    "    model.eval()\n",
    "    batch_size = src_ids.size(0)\n",
    "    \n",
    "    bos_token_id = getattr(tokenizer, 'bos_token_id', tokenizer.cls_token_id)\n",
    "    eos_token_id = getattr(tokenizer, 'eos_token_id', tokenizer.sep_token_id)\n",
    "    \n",
    "        \n",
    "    if batch_size > 1:\n",
    "        return [\n",
    "            beam_search_decode(\n",
    "                model, src_ids[i:i+1],\n",
    "                None if src_mask is None else src_mask[i:i+1],\n",
    "                max_len, beam_size, device, tokenizer, length_penalty, early_stopping\n",
    "            )\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "    \n",
    "    current_tokens = torch.full(\n",
    "        (beam_size, 1), bos_token_id, dtype=torch.long, device=device\n",
    "    )\n",
    "    beam_scores = torch.zeros(beam_size, device=device)\n",
    "    done_beams = [False] * beam_size\n",
    "    \n",
    "    expanded_src_ids  = src_ids.expand(beam_size, -1)\n",
    "    expanded_src_mask = None if src_mask is None else src_mask.expand(beam_size, -1)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        tgt_mask = None\n",
    "        if hasattr(model, 'generate_square_subsequent_mask'):\n",
    "            tgt_mask = model.generate_square_subsequent_mask(\n",
    "                current_tokens.size(1)\n",
    "            ).to(device)\n",
    "\n",
    "        with torch.inference_mode():   \n",
    "            outputs = model(\n",
    "                src=expanded_src_ids,\n",
    "                tgt=current_tokens,\n",
    "                src_attention_mask=expanded_src_mask,\n",
    "                tgt_attention_mask=tgt_mask\n",
    "            )\n",
    "        \n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        next_token_logprobs = F.log_softmax(next_token_logits, dim=-1)\n",
    "       \n",
    "        vocab_size = next_token_logprobs.size(-1)\n",
    "        expanded_scores = beam_scores.unsqueeze(1) + next_token_logprobs \n",
    "        flat_scores     = expanded_scores.view(-1)                      \n",
    "        \n",
    "        topk_scores, topk_indices = torch.topk(\n",
    "            flat_scores, k=min(2 * beam_size, flat_scores.size(0))\n",
    "        )\n",
    "        beam_ix = topk_indices // vocab_size\n",
    "        token_ix = topk_indices % vocab_size\n",
    "        \n",
    "        candidates = []\n",
    "        for b, tok, sc in zip(beam_ix.tolist(), token_ix.tolist(), topk_scores.tolist()):\n",
    "            if done_beams[b]:\n",
    "                continue\n",
    "            new_tokens = torch.cat([\n",
    "                current_tokens[b],\n",
    "                torch.tensor([tok], dtype=torch.long, device=device)\n",
    "            ], dim=0)\n",
    "            candidates.append({\n",
    "                'tokens': new_tokens,\n",
    "                'score': sc,\n",
    "                'is_done': (tok == eos_token_id)\n",
    "            })\n",
    "            if len(candidates) >= beam_size:\n",
    "                break\n",
    "        \n",
    "        if all(done_beams) and early_stopping:\n",
    "            break\n",
    "        \n",
    "        while len(candidates) < beam_size:\n",
    "            candidates.append(candidates[0])\n",
    "        \n",
    "        current_tokens = torch.stack([c['tokens'] for c in candidates])\n",
    "        beam_scores    = torch.tensor([c['score'] for c in candidates], device=device)\n",
    "        done_beams     = [c['is_done'] for c in candidates]\n",
    "        \n",
    "        if all(done_beams) and early_stopping:\n",
    "            break\n",
    "    \n",
    "    seq_lens = current_tokens.size(1)\n",
    "    adjusted_scores = beam_scores / (seq_lens ** length_penalty)\n",
    "    best_idx  = adjusted_scores.argmax().item()\n",
    "    best_tokens = current_tokens[best_idx].tolist()\n",
    "    \n",
    "    return tokenizer.decode(best_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask=None, max_len=100, device=None, tokenizer=None):\n",
    "    \n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = src.device\n",
    "\n",
    "    bos_token_id = getattr(tokenizer, 'bos_token_id', tokenizer.cls_token_id)\n",
    "    eos_token_id = getattr(tokenizer, 'eos_token_id', tokenizer.sep_token_id)\n",
    "\n",
    "    decoder_input = torch.tensor([[bos_token_id]], device=device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for _ in range(max_len - 1):\n",
    "            logits = model(\n",
    "                src=src,\n",
    "                tgt=decoder_input,\n",
    "                src_attention_mask=src_mask,\n",
    "                tgt_attention_mask=None\n",
    "            )\n",
    "            next_logits = logits[:, -1, :]\n",
    "            next_token  = next_logits.argmax(dim=-1, keepdim=True)\n",
    "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, src_ids, src_mask=None, max_len=100, \n",
    "                     method=\"beam_search\", beam_size=5, device=None):\n",
    "                     \n",
    "    if method == \"greedy\":\n",
    "        gen_ids = greedy_decode(\n",
    "            model, src_ids, src_mask, max_len, device, tokenizer\n",
    "        )\n",
    "        return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    elif method == \"beam_search\":\n",
    "        return beam_search_decode(\n",
    "            model, src_ids, src_mask, max_len, beam_size, device, tokenizer\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoding method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model weights loaded successfully\")\n",
    "        \n",
    "        if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"Optimizer state restored\")\n",
    "        \n",
    "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            print(\"Scheduler state restored\")\n",
    "        \n",
    "        metrics = {\n",
    "            'train_loss': checkpoint.get('train_loss'),\n",
    "            'val_loss': checkpoint.get('val_loss'),\n",
    "            'rouge_scores': checkpoint.get('rouge_scores')\n",
    "        }\n",
    "        \n",
    "        epoch = checkpoint.get('epoch', -1) + 1  # +1 because we want to start from the next epoch\n",
    "        \n",
    "        print(f\"Checkpoint loaded from epoch {epoch}\")\n",
    "        if 'val_loss' in checkpoint:\n",
    "            print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "        \n",
    "        return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The PyTorch API of nested tensors is in prototype stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:40<00:00, 26.97it/s, loss=4.1639]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.1565 | Val Loss: 4.8470\n",
      "Saved checkpoint for epoch 1\n",
      "\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:39<00:00, 27.00it/s, loss=3.7121]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.4710 | Val Loss: 3.9820\n",
      "Saved checkpoint for epoch 2\n",
      "\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:17<00:00, 27.41it/s, loss=3.0504]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8809 | Val Loss: 3.6080\n",
      "Saved checkpoint for epoch 3\n",
      "\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:27<00:00, 27.23it/s, loss=3.1621]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.5465 | Val Loss: 3.4076\n",
      "Saved checkpoint for epoch 4\n",
      "\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:28<00:00, 27.21it/s, loss=3.9386]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3396 | Val Loss: 3.2916\n",
      "Saved checkpoint for epoch 5\n",
      "\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:36<00:00, 27.05it/s, loss=4.9369]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1951 | Val Loss: 3.2235\n",
      "Saved checkpoint for epoch 6\n",
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:24<00:00, 27.28it/s, loss=1.6823]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.0816 | Val Loss: 3.1709\n",
      "Saved checkpoint for epoch 7\n",
      "\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:33<00:00, 27.10it/s, loss=3.8146]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9858 | Val Loss: 3.1401\n",
      "Saved checkpoint for epoch 8\n",
      "\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:40<00:00, 26.98it/s, loss=1.9707]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.9049 | Val Loss: 3.1094\n",
      "Saved checkpoint for epoch 9\n",
      "\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [24:13<00:00, 26.35it/s, loss=1.8389]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8303 | Val Loss: 3.0883\n",
      "Saved checkpoint for epoch 10\n",
      "\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [24:27<00:00, 26.11it/s, loss=2.9861]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 70.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7626 | Val Loss: 3.0714\n",
      "Saved checkpoint for epoch 11\n",
      "\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:53<00:00, 26.73it/s, loss=2.2004]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 70.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6987 | Val Loss: 3.0611\n",
      "Saved checkpoint for epoch 12\n",
      "\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:38<00:00, 27.01it/s, loss=1.8714]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.6389 | Val Loss: 3.0563\n",
      "Saved checkpoint for epoch 13\n",
      "\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:29<00:00, 27.18it/s, loss=1.6905]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5823 | Val Loss: 3.0514\n",
      "Saved checkpoint for epoch 14\n",
      "\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:32<00:00, 27.12it/s, loss=2.5579]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5269 | Val Loss: 3.0493\n",
      "Saved checkpoint for epoch 15\n",
      "\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:42<00:00, 26.93it/s, loss=0.8846]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4739 | Val Loss: 3.0459\n",
      "Saved checkpoint for epoch 16\n",
      "\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:39<00:00, 26.99it/s, loss=2.2711]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4232 | Val Loss: 3.0472\n",
      "Saved checkpoint for epoch 17\n",
      "\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:44<00:00, 26.89it/s, loss=2.7493]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3729 | Val Loss: 3.0487\n",
      "Saved checkpoint for epoch 18\n",
      "\n",
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:51<00:00, 26.77it/s, loss=2.3175]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3248 | Val Loss: 3.0540\n",
      "Saved checkpoint for epoch 19\n",
      "\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [24:03<00:00, 26.55it/s, loss=1.3998]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 68.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2777 | Val Loss: 3.0587\n",
      "Saved checkpoint for epoch 20\n",
      "\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [24:02<00:00, 26.56it/s, loss=1.8405]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2322 | Val Loss: 3.0600\n",
      "Saved checkpoint for epoch 21\n",
      "\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:49<00:00, 26.80it/s, loss=0.5197]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1881 | Val Loss: 3.0662\n",
      "Saved checkpoint for epoch 22\n",
      "\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [24:12<00:00, 26.38it/s, loss=2.2824]\n",
      "Validating: 100%|██████████| 1442/1442 [00:21<00:00, 68.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1457 | Val Loss: 3.0682\n",
      "Saved checkpoint for epoch 23\n",
      "\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:55<00:00, 26.69it/s, loss=3.1005]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 69.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1064 | Val Loss: 3.0701\n",
      "Saved checkpoint for epoch 24\n",
      "\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 38316/38316 [23:57<00:00, 26.66it/s, loss=2.2588]\n",
      "Validating: 100%|██████████| 1442/1442 [00:20<00:00, 70.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0708 | Val Loss: 3.0722\n",
      "Saved checkpoint for epoch 25\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, train_loader, valid_loader, vocab_size, tokenizer, optimizer, criterion, scheduler, pad_token_id, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion, device, vocab_size, pad_token_id, tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    full_rouge_scores = []\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in progress_bar:\n",
    "            src = batch['input_ids'].to(device, non_blocking=True)\n",
    "            src_attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            tgt = batch['labels'].to(device, non_blocking=True)\n",
    "            tgt_inp, tgt_lbl = tgt[:, :-1], tgt[:, 1:]\n",
    "            \n",
    "            tgt_attention_mask = (tgt_inp != pad_token_id).to(device, non_blocking=True)\n",
    "            loss_mask = (tgt_lbl != pad_token_id).float()\n",
    "            \n",
    "            logits = model(\n",
    "                src=src,\n",
    "                tgt=tgt_inp,\n",
    "                src_attention_mask=src_attention_mask,\n",
    "                tgt_attention_mask=tgt_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(logits.reshape(-1, vocab_size), tgt_lbl.reshape(-1))\n",
    "            masked_loss = (loss * loss_mask.reshape(-1)).sum() / max(loss_mask.sum(), 1)\n",
    "            total_loss += masked_loss.item()\n",
    "            \n",
    "            batch_size = src.size(0)\n",
    "            for i in range(batch_size):\n",
    "                summary = generate_summary(\n",
    "                    model=model,\n",
    "                    src_ids=src[i:i+1],\n",
    "                    src_mask=src_attention_mask[i:i+1],\n",
    "                    max_len=100,\n",
    "                    method=\"beam_search\",\n",
    "                    beam_size=5,\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                reference = tokenizer.decode(\n",
    "                    [t for t in tgt[i].tolist() if t != pad_token_id], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                \n",
    "                scores = scorer.score(reference, summary)\n",
    "                full_rouge_scores.append({\n",
    "                    'reference': reference,\n",
    "                    'summary': summary,\n",
    "                    'scores': scores\n",
    "                })\n",
    "                \n",
    "                for k in rouge_scores:\n",
    "                    rouge_scores[k].append(scores[k].fmeasure)\n",
    "                \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_rouge = {k: sum(v)/max(len(v), 1) for k, v in rouge_scores.items()}\n",
    "    \n",
    "    return avg_loss, avg_rouge, full_rouge_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_checkpoints(checkpoint_paths, model_class, model_kwargs, val_loader, \n",
    "                         criterion, device, vocab_size, pad_token_id, tokenizer):\n",
    "    results = []\n",
    "\n",
    "    for ckpt_path in checkpoint_paths:\n",
    "        print(f\"\\n--- Evaluating {ckpt_path} ---\")\n",
    "        \n",
    "        model = model_class(**model_kwargs).to(device)\n",
    "        model, _, _, epoch, metrics = load_checkpoint(ckpt_path, model)\n",
    "\n",
    "        val_loss, avg_rouge, full_rouge_scores = evaluate_model(\n",
    "            model=model,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            vocab_size=vocab_size,\n",
    "            pad_token_id=pad_token_id,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}] ValLoss: {val_loss:.4f} | \"\n",
    "              f\"ROUGE-1: {avg_rouge['rouge1']:.4f} | \"\n",
    "              f\"ROUGE-2: {avg_rouge['rouge2']:.4f} | \"\n",
    "              f\"ROUGE-L: {avg_rouge['rougeL']:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"checkpoint\": ckpt_path,\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"rouge\": avg_rouge,\n",
    "            \"full_rouge_scores\": full_rouge_scores\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src_ids, src_mask=None, max_len=100, beam_size=5, \n",
    "                       device='cpu', tokenizer=None, length_penalty=1.0, \n",
    "                       early_stopping=True):\n",
    "    \n",
    "    model.eval()\n",
    "    batch_size = src_ids.size(0)\n",
    "\n",
    "    bos_token_id = getattr(tokenizer, 'bos_token_id', tokenizer.cls_token_id)\n",
    "    eos_token_id = getattr(tokenizer, 'eos_token_id', tokenizer.sep_token_id)\n",
    "    \n",
    "    if batch_size > 1:\n",
    "        return [\n",
    "            beam_search_decode(\n",
    "                model, src_ids[i:i+1],\n",
    "                None if src_mask is None else src_mask[i:i+1],\n",
    "                max_len, beam_size, device, tokenizer, length_penalty, early_stopping\n",
    "            )\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "\n",
    "    current_tokens = torch.full((beam_size, 1), bos_token_id, dtype=torch.long, device=device)\n",
    "    beam_scores = torch.zeros(beam_size, device=device)\n",
    "    done_beams = [False] * beam_size\n",
    "\n",
    "    expanded_src_ids = src_ids.expand(beam_size, -1)\n",
    "    expanded_src_mask = None if src_mask is None else src_mask.expand(beam_size, -1)\n",
    "\n",
    "    for step in range(max_len - 1):\n",
    "        tgt_mask = None\n",
    "        if hasattr(model, 'generate_square_subsequent_mask'):\n",
    "            tgt_mask = model.generate_square_subsequent_mask(\n",
    "                current_tokens.size(1)\n",
    "            ).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(\n",
    "                src=expanded_src_ids,\n",
    "                tgt=current_tokens,\n",
    "                src_attention_mask=expanded_src_mask,\n",
    "                tgt_attention_mask=tgt_mask\n",
    "            )\n",
    "\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        next_token_logprobs = F.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        vocab_size = next_token_logprobs.size(-1)\n",
    "        expanded_scores = beam_scores.unsqueeze(1) + next_token_logprobs\n",
    "        flat_scores = expanded_scores.view(-1)\n",
    "\n",
    "        topk_scores, topk_indices = torch.topk(\n",
    "            flat_scores, k=min(2 * beam_size, flat_scores.size(0))\n",
    "        )\n",
    "        beam_ix = topk_indices // vocab_size\n",
    "        token_ix = topk_indices % vocab_size\n",
    "\n",
    "        candidates = []\n",
    "        for b, tok, sc in zip(beam_ix.tolist(), token_ix.tolist(), topk_scores.tolist()):\n",
    "            new_tokens = torch.cat([\n",
    "                current_tokens[b],\n",
    "                torch.tensor([tok], dtype=torch.long, device=device)\n",
    "            ], dim=0)\n",
    "            candidates.append({\n",
    "                'tokens': new_tokens,\n",
    "                'score': sc,\n",
    "                'is_done': done_beams[b] or (tok == eos_token_id)\n",
    "            })\n",
    "            if len(candidates) >= beam_size:\n",
    "                break\n",
    "\n",
    "        if len(candidates) == 0:\n",
    "            return tokenizer.decode([bos_token_id], skip_special_tokens=True)\n",
    "\n",
    "        while len(candidates) < beam_size:\n",
    "            candidates.append(candidates[0])\n",
    "\n",
    "        current_tokens = torch.stack([c['tokens'] for c in candidates])\n",
    "        beam_scores = torch.tensor([c['score'] for c in candidates], device=device)\n",
    "        done_beams = [c['is_done'] for c in candidates]\n",
    "\n",
    "        if all(done_beams) and early_stopping:\n",
    "            break\n",
    "\n",
    "    seq_lens = current_tokens.size(1)\n",
    "    adjusted_scores = beam_scores / (seq_lens ** length_penalty)\n",
    "    best_idx = adjusted_scores.argmax().item()\n",
    "    best_tokens = current_tokens[best_idx].tolist()\n",
    "\n",
    "    return tokenizer.decode(best_tokens, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sample_summary(model, val_loader, tokenizer, device, \n",
    "                                   max_len=100, method='beam_search', beam_size=5):\n",
    "    model.eval()\n",
    "    \n",
    "    data_iter = iter(val_loader)\n",
    "    batch = None\n",
    "    for _ in range(random.randint(0, len(val_loader)-1)):\n",
    "        batch = next(data_iter)\n",
    "        \n",
    "    if batch is None:\n",
    "        print(\"Validation loader is empty or failed to sample.\")\n",
    "        return\n",
    "\n",
    "    src = batch['input_ids'].to(device)\n",
    "    tgt = batch['labels'].to(device)\n",
    "    \n",
    "    src = src.to(device)\n",
    "    tgt = tgt.to(device)\n",
    "\n",
    "    i = random.randint(0, src.size(0) - 1)\n",
    "    src_sample = src[i:i+1]\n",
    "    tgt_sample = tgt[i:i+1]\n",
    "\n",
    "    src_mask = (src_sample == tokenizer.pad_token_id)\n",
    "\n",
    "    summary = generate_summary(\n",
    "        model=model,\n",
    "        src_ids=src_sample,\n",
    "        src_mask=src_mask,\n",
    "        max_len=max_len,\n",
    "        method=method,\n",
    "        beam_size=beam_size,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    input_text = tokenizer.decode(\n",
    "        [t for t in src_sample[0].tolist() if t != tokenizer.pad_token_id],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    reference_summary = tokenizer.decode(\n",
    "        [t for t in tgt_sample[0].tolist() if t != tokenizer.pad_token_id],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n📘 Input Document:\")\n",
    "    print(input_text)\n",
    "    print(\"\\n✅ Reference Summary:\")\n",
    "    print(reference_summary)\n",
    "    print(\"\\n📝 Generated Summary:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    enc_layers=3,\n",
    "    dec_layers=3,\n",
    "    dim_ff=2048,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from checkpoint/transformer_epoch_16.pt\n",
      "Model weights loaded successfully\n",
      "Checkpoint loaded from epoch 16\n",
      "Validation loss: 3.0459\n",
      "Model loaded from epoch 16\n"
     ]
    }
   ],
   "source": [
    "model, _, _, epoch, _ = load_checkpoint(\"checkpoint/transformer_epoch_16.pt\", model)\n",
    "print(f\"Model loaded from epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Input Document:\n",
      "The study by Family and Childcare Trust and Children in Scotland said there was an 80% variation in the cost of nursery care and a 92% variation for over-5s. It indicated that while the price of nursery care appeared to have stabilised, out of school care costs had increased.  The Scottish Childcare Report covered the year from December 2011 to 2012. As well as sharp variations in costs, it also found that about 40% of local authorities did not know if they had sufficient childcare for working parents, making childcare a postcode lottery for parents.  Of the councils that had some knowledge about the supply of childcare in their local area, there was a particular shortage of childcare for older children and disabled children, it claimed.    The report said more than half of all families in Scotland had used grandparents for childcare purposes in the past six months - the highest proportion in the UK. Children in Scotland Policy Officer Jim Stephen said: We welcome the findings that childcare costs for young children have largely stabilised, but are concerned that the costs of out of school care appears to have increased. The research found that nursery care for a child under two-years-old in Scotland cost an average of 100 per week.  A parent buying 50 hours of care each per week could face an annual bill of nearly 11,000.  The highest fees in Scotland were 235 per week for 25 hours, it said. The report also found that childcare costs at an after-school club for those aged over five averaged 50.46 per week, which was a 3.9% increase on the previous year - higher than the rate of inflation. This was more than in England and Wales where the costs were 49.71 and 48.46 respectively. Based on full time attendance, a parent with two children of primary school age could face a bill of 4,000 per year for after-school care during term time.  The data was collected under the promise of anonymity for councils to avoid league tables and comparisons that do not reflect the complexity of the childcare market. The report... Tóm tắt bài đã cho\n",
      "\n",
      "✅ Reference Summary:\n",
      "The cost of nursery care and out-of-school care varies significantly across Scotland, according to a new report.\n",
      "\n",
      "📝 Generated Summary:\n",
      "The cost of childcare costs in Scotland has risen by almost a third in the past year, according to a new report.\n"
     ]
    }
   ],
   "source": [
    "generate_random_sample_summary(\n",
    "    model=model,\n",
    "    val_loader=valid_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_len=100,\n",
    "    method=\"beam_search\",\n",
    "    beam_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Input Document:\n",
      "Jarring photos of facilities in the Rio Grande show 51 female migrants held in a cell made for 40 men, and 71 males held in a cell built for 41 women.  Adults were packed in standing room only cells for a week, with others held in overcrowded cells for over a month.  One facility manager called the situation a ticking time bomb.  We are concerned that overcrowding and prolonged detention represent an immediate risk to the health and safety of [Department of Homeland Security] agents and officers, and to those detained, inspectors said in the report.  The inspectors, from the US inspector general, visited seven sites throughout the Rio Grande valley in southern Texas.  At the facilities, the inspectors found that 30% of the detained children had been held for longer than the 72 hours permitted. Some had no access to showers or hot meals and had little access to clean clothes.  When detainees observed us, they banged on the cell windows, shouted, pressed notes to the window with their time in custody, and gestured to evidence of their time in custody, like facial hair, the report said.  They described detainees clogging toilets with blankets and socks in order to be released while the cells were fixed.  The report says these conditions directly contradict the US Customs and Border Protections (CBP) own standards. The inspectors called upon the Department of Homeland Security (DHS) to take immediate steps to alleviate dangerous overcrowding.   According to the CBP, the Rio Grande has the highest volume of migrants on the southwest border, recording almost 250,000 apprehensions so far this year - marking a 124% increase from 2018. On Tuesday, the DHS said they would build two tents to house additional migrants by the end of July. The agency added that fewer children are in their care than earlier this month. The DHS had 2,800 children in detention on 7 June, according to government figures. By 25 June, less than 1,000 were in custody, it said. In recent weeks, conditions at these facilities have been at the... Cho tôi nội dung chính của bài văn đã cho\n",
      "\n",
      "✅ Reference Summary:\n",
      "A report from an internal US watchdog has found dangerous overcrowding in migrant detention centres in the south and urged authorities to act.\n",
      "\n",
      "📝 Generated Summary:\n",
      "The US border agency has released images of a detention centre in Texas that has been detained in the US for more than a month.\n"
     ]
    }
   ],
   "source": [
    "generate_random_sample_summary(\n",
    "    model=model,\n",
    "    val_loader=valid_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_len=100,\n",
    "    method=\"greedy\",\n",
    "    beam_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_from_text(model, tokenizer, device, src_text, max_len=100, method='beam_search', beam_size=5):\n",
    "    model.eval()\n",
    "    \n",
    "    src = tokenizer.encode(src_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    src_mask = (src == tokenizer.pad_token_id)\n",
    "\n",
    "    summary = generate_summary(\n",
    "        model=model,\n",
    "        src_ids=src,\n",
    "        src_mask=src_mask,\n",
    "        max_len=max_len,\n",
    "        method=method,\n",
    "        beam_size=beam_size,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    input_text = tokenizer.decode(src[0], skip_special_tokens=True)\n",
    "    print(\"\\n📘 Input Document:\")\n",
    "    print(input_text)\n",
    "    print(\"\\n📝 Generated Summary:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Input Document:\n",
      "The field of artificial intelligence has seen rapid advancements in recent years. Researchers are exploring new ways to improve machine learning algorithms, particularly in natural language processing.\n",
      "\n",
      "📝 Generated Summary:\n",
      "A team of scientists has been created to help scientists in Jersey to improve the language of the language.  \n"
     ]
    }
   ],
   "source": [
    "src_text = \"XYZ University is widely recognized as one of the nation’s top-tier institutions for advanced research, education, and innovation in computer science, engineering, and interdisciplinary studies. Each year, the university admits a select cohort of only 30 exceptionally talented and driven students into its prestigious research fellowship program. This limited intake ensures high-quality mentorship and individual attention, making the selection process intensely competitive. These fellows are guided by globally recognized faculty who are pioneers in their respective domains. Faculty members at XYZ University are deeply involved in groundbreaking research across a wide range of fields, including Artificial Intelligence, Machine Learning, Quantum Computing, Computational Neuroscience, Human-Computer Interaction, Software Verification, Natural Language Processing, and Computer Vision.\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
