{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c160157d-f383-4dbd-beae-d03c9a796d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, get_linear_schedule_with_warmup\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3594a4a3-7ef3-4b1d-acea-b8178e4177de",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_len   = 512      \n",
    "max_tgt_len   = 128      \n",
    "batch_size    = 8\n",
    "epochs        = 25\n",
    "lr            = 0.0001\n",
    "weight_decay  = 0.01\n",
    "grad_clip     = 1.0\n",
    "beam_size     = 4\n",
    "warmup_ratio  = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c31b7e4-6848-46f8-b5cd-d6ae108d6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007ca6f6-2d49-42fa-8887-3b6f10147d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.pos = nn.Parameter(torch.zeros(max_len, d_model))\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        return x + self.pos[:seq_len].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb99c30-78af-4b6e-93f0-4aaccf54e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSummarizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, enc_layers=3, dec_layers=3, dim_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed       = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc     = TrainablePositionalEncoding(d_model)\n",
    "        self.pos_dec     = TrainablePositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, enc_layers, dec_layers, dim_ff, dropout, activation='gelu', batch_first=True)\n",
    "        self.out_proj    = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_attention_mask=None, tgt_attention_mask=None):\n",
    "\n",
    "        pad_id = tokenizer.pad_token_id\n",
    "        src_kpm = (src == pad_id)\n",
    "        tgt_kpm = (tgt == pad_id)\n",
    "        \n",
    "        src_emb = self.pos_enc(self.embed(src) * math.sqrt(self.embed.embedding_dim))\n",
    "        tgt_emb = self.pos_dec(self.embed(tgt) * math.sqrt(self.embed.embedding_dim))\n",
    "        \n",
    "        size = tgt.size(1)\n",
    "        tgt_mask = torch.triu(torch.full((size, size), True,dtype=torch.bool), 1).to(device)\n",
    "        \n",
    "        out = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_kpm,\n",
    "                               tgt_key_padding_mask=tgt_kpm,\n",
    "                               memory_key_padding_mask=src_kpm\n",
    "                              )\n",
    "        \n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc370a2-33a6-493a-8716-7f9c036dd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edae9ba5-5d55-4645-9dc1-a6c0a0c4a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSummarizer(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4c65d6-7395-4d4e-a0e6-007539ae5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f519b47a-68eb-4faa-993a-c51fe0bdb8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 74,616,921\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trainable parameters: {count_trainable_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440f7e6-80c1-4c98-b723-69d8299e2249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
